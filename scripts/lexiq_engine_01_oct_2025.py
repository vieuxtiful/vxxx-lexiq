# -*- coding: utf-8 -*-
"""LexiQ - Engine (01 Oct 2025).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13cOuU5SGdP2ZP7w1dRiRycZjIfEN9Nmw
"""

#!/usr/bin/env python3
"""
Complete Enhanced LexiQ Engine with Language-Specific Semantic Type Labeling
- English: Shows "Semantic Type"
- Non-English languages: Shows "Definition"

This is the full standalone program integrating all enhancements.
"""

import json
import time
import re
import hashlib
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Mock heavy dependencies for standalone operation
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sentence_transformers import SentenceTransformer
    import redis
    DEPENDENCIES_AVAILABLE = True
except ImportError:
    logger.warning("Heavy dependencies not available, using mock implementations")
    DEPENDENCIES_AVAILABLE = False

    class TfidfVectorizer:
        def __init__(self, **kwargs):
            pass
        def fit_transform(self, texts):
            return [[0.1, 0.2, 0.3] for _ in texts]

    class SentenceTransformer:
        def __init__(self, model_name):
            self.model_name = model_name
        def encode(self, text):
            return [0.1] * 384

    class redis:
        class Redis:
            def __init__(self, **kwargs):
                self.data = {}
            def ping(self):
                return True
            def setex(self, key, expire, value):
                self.data[key] = value
                return True
            def get(self, key):
                return self.data.get(key)

# Enhanced Data Classes
@dataclass
class SemanticTypeInfo:
    """Semantic type information with language-specific UI labeling"""
    semantic_type: str
    confidence: float
    ui_information: Dict[str, Any]
    language: str

@dataclass
class GrammarIssue:
    """Grammar issue information"""
    rule: str
    severity: str
    start_pos: int
    end_pos: int
    matched_text: str
    suggestion: str

@dataclass
class AnalyzedTerm:
    """Enhanced analyzed term with semantic types and grammar issues"""
    text: str
    startPosition: int
    endPosition: int
    classification: str
    score: float
    frequency: int
    context: str
    rationale: str
    suggestions: Optional[List[str]] = None
    semantic_type: Optional[Dict[str, Any]] = None
    grammar_issues: Optional[List[Dict[str, Any]]] = None
    ui_metadata: Optional[Dict[str, Any]] = None

@dataclass
class AnalysisStatistics:
    """Enhanced analysis statistics with grammar metrics"""
    totalTerms: int
    validTerms: int
    reviewTerms: int
    criticalTerms: int
    qualityScore: float
    confidenceMin: float
    confidenceMax: float
    coverage: float
    grammarScore: Optional[float] = None
    grammarIssues: Optional[int] = None
    spellingIssues: Optional[int] = None

@dataclass
class GrammarAnalysis:
    """Grammar analysis results"""
    text: str
    language: str
    issues: List[Dict[str, Any]]
    grammar_score: float
    total_issues: int
    issue_breakdown: Dict[str, int]

@dataclass
class AnalysisResult:
    """Complete analysis result"""
    terms: List[AnalyzedTerm]
    statistics: AnalysisStatistics
    grammar_analysis: Optional[GrammarAnalysis] = None
    processing_metadata: Optional[Dict[str, Any]] = None

# Language-Specific Semantic Type System
class LanguageSpecificSemanticTypeSystem:
    """Enhanced Semantic Type System with language-specific UI labeling"""

    def __init__(self):
        # Base semantic type mappings
        self.semantic_types = {
            'Entity': {
                'ui_category': 'semantic_type',
                'color_code': '#4CAF50',  # Green
                'description_en': 'Concrete or abstract objects, people, places, or concepts',
                'description_es': 'Objetos concretos o abstractos, personas, lugares o conceptos',
                'description_fr': 'Objets concrets ou abstraits, personnes, lieux ou concepts',
                'description_de': 'Konkrete oder abstrakte Objekte, Personen, Orte oder Konzepte',
                'keywords': ['noun', 'object', 'person', 'place', 'concept', 'thing'],
                'suffixes': ['er', 'or', 'ist', 'ian', 'ism', 'ity', 'ness']
            },
            'Event': {
                'ui_category': 'semantic_type',
                'color_code': '#FF9800',  # Orange
                'description_en': 'Actions, processes, or states that occur over time',
                'description_es': 'Acciones, procesos o estados que ocurren a lo largo del tiempo',
                'description_fr': 'Actions, processus ou états qui se produisent dans le temps',
                'description_de': 'Handlungen, Prozesse oder Zustände, die über Zeit auftreten',
                'keywords': ['action', 'process', 'event', 'activity', 'procedure'],
                'suffixes': ['tion', 'sion', 'ment', 'ing', 'ance', 'ence']
            },
            'Property': {
                'ui_category': 'semantic_type',
                'color_code': '#2196F3',  # Blue
                'description_en': 'Qualities, attributes, or characteristics of entities',
                'description_es': 'Cualidades, atributos o características de entidades',
                'description_fr': 'Qualités, attributs ou caractéristiques des entités',
                'description_de': 'Eigenschaften, Attribute oder Merkmale von Entitäten',
                'keywords': ['quality', 'attribute', 'characteristic', 'feature', 'property'],
                'suffixes': ['able', 'ible', 'ous', 'ful', 'less', 'ly']
            },
            'Agent': {
                'ui_category': 'semantic_type',
                'color_code': '#9C27B0',  # Purple
                'description_en': 'Entities that can perform actions or cause events',
                'description_es': 'Entidades que pueden realizar acciones o causar eventos',
                'description_fr': 'Entités qui peuvent effectuer des actions ou causer des événements',
                'description_de': 'Entitäten, die Handlungen ausführen oder Ereignisse verursachen können',
                'keywords': ['actor', 'agent', 'performer', 'doer', 'operator'],
                'suffixes': ['er', 'or', 'ant', 'ent', 'ist', 'ian']
            }
        }

        # Language-specific UI labels
        self.ui_labels = {
            'en': {
                'main_label': 'Semantic Type',
                'badge_text': 'Semantic Type',
                'tooltip_prefix': 'Semantic Type:',
                'display_names': {
                    'Entity': 'Entity Type',
                    'Event': 'Event Type',
                    'Property': 'Property Type',
                    'Agent': 'Agent Type'
                }
            },
            'es': {
                'main_label': 'Significado',
                'badge_text': 'Significado',
                'tooltip_prefix': 'Significado:',
                'display_names': {
                    'Entity': 'Significado de Entidad',
                    'Event': 'Significado de Proceso',
                    'Property': 'Significado de Atributo',
                    'Agent': 'Significado de Actor'
                }
            },
            'fr': {
                'main_label': 'Signification',
                'badge_text': 'Signification',
                'tooltip_prefix': 'Signification:',
                'display_names': {
                    'Entity': 'Signification d\'Entité',
                    'Event': 'Signification de Processus',
                    'Property': 'Signification d\'Attribut',
                    'Agent': 'Signification d\'Acteur'
                }
            },
            'de': {
                'main_label': 'Bedeutung',
                'badge_text': 'Bedeutung',
                'tooltip_prefix': 'Bedeutung:',
                'display_names': {
                    'Entity': 'Entitätsbedeutung',
                    'Event': 'Prozessbedeutung',
                    'Property': 'Attributbedeutung',
                    'Agent': 'Akteursbedeutung'
                }
            },
            'pt': {
                'main_label': 'Significado',
                'badge_text': 'Significado',
                'tooltip_prefix': 'Significado:',
                'display_names': {
                    'Entity': 'Significado de Entidade',
                    'Event': 'Significado de Processo',
                    'Property': 'Significado de Atributo',
                    'Agent': 'Significado de Ator'
                }
            },
            'it': {
                'main_label': 'Significato',
                'badge_text': 'Significato',
                'tooltip_prefix': 'Significato:',
                'display_names': {
                    'Entity': 'Significato di Entità',
                    'Event': 'Significato di Processo',
                    'Property': 'Significato di Attributo',
                    'Agent': 'Significato di Attore'
                }
            },
            'ja': {
                'main_label': '意味',
                'badge_text': '意味',
                'tooltip_prefix': '意味:',
                'display_names': {
                    'Entity': 'エンティティ意味',
                    'Event': 'プロセス意味',
                    'Property': '属性意味',
                    'Agent': 'アクター意味'
                }
            },
            'zh': {
                'main_label': '意义',
                'badge_text': '意义',
                'tooltip_prefix': '意义:',
                'display_names': {
                    'Entity': '实体意义',
                    'Event': '过程意义',
                    'Property': '属性意义',
                    'Agent': '行为者意义'
                }
            }
        }

    def infer_semantic_type(self, term: str, language: str = 'en', context: str = '') -> SemanticTypeInfo:
        """Infer semantic type with language-specific UI labeling"""
        term_lower = term.lower().strip()
        confidence = 0.5

        # Calculate type scores
        type_scores = {}
        for sem_type, info in self.semantic_types.items():
            score = 0.0

            # Suffix matching
            for suffix in info['suffixes']:
                if term_lower.endswith(suffix):
                    score += 0.3
                    break

            # Context keyword matching
            context_lower = context.lower()
            for keyword in info['keywords']:
                if keyword in context_lower:
                    score += 0.2
                    break

            # Length and complexity bonus
            if len(term) > 8:
                score += 0.1
            if '-' in term or '_' in term:
                score += 0.1

            type_scores[sem_type] = score

        # Determine best type
        best_type = max(type_scores.items(), key=lambda x: x[1])
        final_type = best_type[0]
        confidence += best_type[1]
        confidence = min(confidence, 0.95)

        # Special rules for common terms
        if term_lower in ['cybersecurity', 'security', 'framework', 'system']:
            final_type = 'Entity'
            confidence = max(confidence, 0.85)
        elif term_lower in ['implementation', 'validation', 'testing', 'analysis']:
            final_type = 'Event'
            confidence = max(confidence, 0.85)
        elif term_lower in ['comprehensive', 'critical', 'important', 'necessary']:
            final_type = 'Property'
            confidence = max(confidence, 0.80)

        # Create language-specific UI information
        ui_info = self._create_ui_information(final_type, language, confidence)

        return SemanticTypeInfo(
            semantic_type=final_type,
            confidence=confidence,
            ui_information=ui_info,
            language=language
        )

    def _create_ui_information(self, semantic_type: str, language: str, confidence: float) -> Dict[str, Any]:
        """Create language-specific UI information"""
        type_info = self.semantic_types[semantic_type]
        lang_labels = self.ui_labels.get(language, self.ui_labels['en'])

        # Get localized description
        description_key = f'description_{language}'
        description = type_info.get(description_key, type_info['description_en'])

        return {
            'category': type_info['ui_category'],
            'color_code': type_info['color_code'],
            'description': description,
            'display_name': lang_labels['display_names'][semantic_type],
            'ui_label': lang_labels['main_label'],
            'badge_text': lang_labels['badge_text'],
            'tooltip_prefix': lang_labels['tooltip_prefix'],
            'semantic_category': semantic_type,
            'confidence_level': self._get_confidence_level(confidence),
            'language': language,
            'visual_indicator': {
                'color': type_info['color_code'],
                'shape': 'circle',
                'size': 'small'
            }
        }

    def _get_confidence_level(self, confidence: float) -> str:
        """Convert numeric confidence to descriptive level"""
        if confidence >= 0.8:
            return 'high'
        elif confidence >= 0.6:
            return 'medium'
        else:
            return 'low'

# Enhanced Grammar Analysis System
class EnhancedGrammarAnalyzer:
    """Advanced grammar analysis with multiple language support"""

    def __init__(self):
        self.grammar_rules = {
            'en': {
                'subject_verb_agreement': [
                    r'\b(he|she|it)\s+(are|were)\b',
                    r'\b(they|we|you)\s+(is|was)\b'
                ],
                'article_usage': [
                    r'\ba\s+[aeiou]',
                    r'\ban\s+[^aeiou]'
                ],
                'spelling_errors': [
                    r'\bcomprehensiv\b',
                    r'\brecieve\b',
                    r'\boccured\b'
                ]
            },
            'es': {
                'concordancia_genero': [
                    r'\bla\s+[a-z]*o\b',
                    r'\bel\s+[a-z]*a\b'
                ],
                'spelling_errors': [
                    r'\bcomprehensiv\b'
                ]
            }
        }

    def analyze_grammar(self, text: str, language: str = 'en') -> GrammarAnalysis:
        """Perform comprehensive grammar analysis"""
        issues = []
        rules = self.grammar_rules.get(language, self.grammar_rules['en'])

        for rule_name, patterns in rules.items():
            for pattern in patterns:
                matches = list(re.finditer(pattern, text, re.IGNORECASE))
                for match in matches:
                    issues.append({
                        'rule': rule_name,
                        'severity': 'medium',
                        'start_pos': match.start(),
                        'end_pos': match.end(),
                        'matched_text': match.group(),
                        'suggestion': self._get_grammar_suggestion(rule_name, match.group(), language)
                    })

        grammar_score = max(0, 100 - (len(issues) * 15))
        issue_breakdown = defaultdict(int)
        for issue in issues:
            issue_breakdown[issue['rule']] += 1

        return GrammarAnalysis(
            text=text,
            language=language,
            issues=issues,
            grammar_score=grammar_score,
            total_issues=len(issues),
            issue_breakdown=dict(issue_breakdown)
        )

    def _get_grammar_suggestion(self, rule_name: str, matched_text: str, language: str) -> str:
        """Generate grammar suggestions"""
        suggestions = {
            'en': {
                'subject_verb_agreement': {
                    'he are': 'he is',
                    'she are': 'she is',
                    'it are': 'it is',
                    'they is': 'they are'
                },
                'spelling_errors': {
                    'comprehensiv': 'comprehensive',
                    'recieve': 'receive',
                    'occured': 'occurred'
                }
            },
            'es': {
                'spelling_errors': {
                    'comprehensiv': 'comprehensive'
                }
            }
        }

        lang_suggestions = suggestions.get(language, suggestions['en'])
        rule_suggestions = lang_suggestions.get(rule_name, {})
        return rule_suggestions.get(matched_text.lower(), f"Check {rule_name.replace('_', ' ')}")

# Enhanced Redis Cache Service
class EnhancedRedisCacheService:
    """Enhanced Redis cache with user preferences support"""

    def __init__(self, host='localhost', port=6379, db=0):
        try:
            self.redis_client = redis.Redis(host=host, port=port, db=db, decode_responses=True)
            self.redis_client.ping()
            self.available = True
            logger.info("Redis cache service initialized successfully")
        except Exception as e:
            logger.warning(f"Redis not available, using in-memory cache: {e}")
            self.redis_client = redis.Redis()  # Mock implementation
            self.available = False

    def set_user_preference(self, user_id: str, key: str, value: Any, expire: int = 86400) -> bool:
        """Set user preference with expiration"""
        try:
            cache_key = f"user_pref:{user_id}:{key}"
            serialized_value = json.dumps(value) if not isinstance(value, str) else value
            return self.redis_client.setex(cache_key, expire, serialized_value)
        except Exception as e:
            logger.error(f"Failed to set user preference: {e}")
            return False

    def get_user_preference(self, user_id: str, key: str, default: Any = None) -> Any:
        """Get user preference with default fallback"""
        try:
            cache_key = f"user_pref:{user_id}:{key}"
            value = self.redis_client.get(cache_key)
            if value is None:
                return default
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return value
        except Exception as e:
            logger.error(f"Failed to get user preference: {e}")
            return default

    def get_user_preferences(self, user_id: str) -> Dict[str, Any]:
        """Get all user preferences"""
        try:
            pattern = f"user_pref:{user_id}:*"
            keys = self.redis_client.keys(pattern)
            preferences = {}
            for key in keys:
                pref_key = key.split(':')[-1]
                value = self.redis_client.get(key)
                try:
                    preferences[pref_key] = json.loads(value)
                except json.JSONDecodeError:
                    preferences[pref_key] = value
            return preferences
        except Exception as e:
            logger.error(f"Failed to get user preferences: {e}")
            return {}

# Main Enhanced LexiQ Engine
class EnhancedLexiQEngine:
    """Complete Enhanced LexiQ Engine with all improvements"""

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.semantic_system = LanguageSpecificSemanticTypeSystem()
        self.grammar_analyzer = EnhancedGrammarAnalyzer()
        self.cache_service = EnhancedRedisCacheService()

        # Performance tracking
        self.performance_stats = {
            'total_analyses': 0,
            'cache_hits': 0,
            'average_processing_time': 0.0,
            'grammar_analyses': 0
        }

        logger.info("Enhanced LexiQ Engine initialized successfully")

    def analyze_text(self, text: str, glossary_content: str = "",
                    language: str = 'en', domain: str = 'general',
                    check_grammar: bool = False, user_id: str = None) -> AnalysisResult:
        """Enhanced analyze_text method with grammar checking flag"""
        start_time = time.time()

        logger.info(f"Analyzing text (Language: {language}, Domain: {domain}, Grammar: {check_grammar})")

        try:
            # Load user preferences if user_id provided
            if user_id:
                user_prefs = self.cache_service.get_user_preferences(user_id)
                language = user_prefs.get('language', language)
                domain = user_prefs.get('domain', domain)
                check_grammar = user_prefs.get('check_grammar', check_grammar)

            # Extract and analyze terms
            terms = self._extract_terms(text)

            # Grammar analysis if requested
            grammar_analysis = None
            if check_grammar:
                grammar_analysis = self.grammar_analyzer.analyze_grammar(text, language)
                self.performance_stats['grammar_analyses'] += 1

            # Analyze each term
            analyzed_terms = []
            for term in terms:
                start_pos = text.find(term)
                end_pos = start_pos + len(term)
                context = self._get_context(text, start_pos, end_pos)

                # Get semantic type information
                semantic_info = self.semantic_system.infer_semantic_type(term, language, context)

                # Check for grammar issues
                term_grammar_issues = []
                if grammar_analysis:
                    term_grammar_issues = [
                        issue for issue in grammar_analysis.issues
                        if issue['start_pos'] <= start_pos <= issue['end_pos']
                    ]

                # Map to UI classification
                classification = self._map_to_ui_classification(
                    semantic_info, term_grammar_issues, language
                )

                analyzed_term = AnalyzedTerm(
                    text=term,
                    startPosition=start_pos,
                    endPosition=end_pos,
                    classification=classification['category'],
                    score=classification['score'],
                    frequency=1,
                    context=context,
                    rationale=classification['rationale'],
                    suggestions=classification.get('suggestions', []),
                    semantic_type=asdict(semantic_info),
                    grammar_issues=term_grammar_issues,
                    ui_metadata=classification.get('ui_metadata', {})
                )

                analyzed_terms.append(analyzed_term)

            # Calculate statistics
            statistics = self._calculate_statistics(analyzed_terms, grammar_analysis)

            # Update performance stats
            processing_time = time.time() - start_time
            self._update_performance_stats(processing_time)

            return AnalysisResult(
                terms=analyzed_terms,
                statistics=statistics,
                grammar_analysis=asdict(grammar_analysis) if grammar_analysis else None,
                processing_metadata={
                    'processing_time': processing_time,
                    'language': language,
                    'domain': domain,
                    'grammar_checked': check_grammar,
                    'user_id': user_id
                }
            )

        except Exception as e:
            logger.error(f"Analysis failed: {e}")
            return AnalysisResult(
                terms=[],
                statistics=AnalysisStatistics(
                    totalTerms=0, validTerms=0, reviewTerms=0, criticalTerms=0,
                    qualityScore=0.0, confidenceMin=0.0, confidenceMax=0.0, coverage=0.0,
                    grammarScore=0.0, grammarIssues=0, spellingIssues=0
                )
            )

    def analyze_grammar(self, text: str, language: str = 'en') -> Dict[str, Any]:
        """Dedicated grammar analysis endpoint"""
        try:
            grammar_analysis = self.grammar_analyzer.analyze_grammar(text, language)
            return asdict(grammar_analysis)
        except Exception as e:
            logger.error(f"Grammar analysis failed: {e}")
            return {
                'error': str(e),
                'text': text,
                'language': language,
                'issues': [],
                'grammar_score': 0.0,
                'total_issues': 0
            }

    def _extract_terms(self, text: str) -> List[str]:
        """Enhanced term extraction"""
        words = re.findall(r'\b\w+(?:[-\']\w+)*\b', text.lower())

        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during',
            'before', 'after', 'above', 'below', 'between', 'among', 'this', 'that',
            'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'
        }

        terms = []
        for word in words:
            if (len(word) > 2 and
                word not in stop_words and
                not word.isdigit()):
                terms.append(word)

        # Remove duplicates while preserving order
        seen = set()
        unique_terms = []
        for term in terms:
            if term not in seen:
                seen.add(term)
                unique_terms.append(term)

        return unique_terms[:20]  # Limit for performance

    def _get_context(self, text: str, start_pos: int, end_pos: int, window: int = 30) -> str:
        """Get context around a term"""
        context_start = max(0, start_pos - window)
        context_end = min(len(text), end_pos + window)
        return text[context_start:context_end].strip()

    def _map_to_ui_classification(self, semantic_info: SemanticTypeInfo,
                                 grammar_issues: List[Dict] = None,
                                 language: str = 'en') -> Dict[str, Any]:
        """Enhanced classification mapping with language-specific labels"""
        confidence = semantic_info.confidence
        has_grammar_issues = bool(grammar_issues)

        if has_grammar_issues:
            category = 'grammar'
            rationale = f"Grammar issues detected: {', '.join([issue['rule'] for issue in grammar_issues])}"
            confidence *= 0.7
        elif confidence >= 0.8:
            category = 'valid'
            label = semantic_info.ui_information['ui_label']
            rationale = f"High confidence term with {label}: {semantic_info.ui_information['display_name']}"
        elif confidence >= 0.6:
            category = 'review'
            label = semantic_info.ui_information['ui_label']
            rationale = f"Medium confidence term requiring review. {label}: {semantic_info.ui_information['display_name']}"
        elif confidence >= 0.4:
            category = 'critical'
            label = semantic_info.ui_information['ui_label']
            rationale = f"Low confidence term requiring attention. {label}: {semantic_info.ui_information['display_name']}"
        else:
            category = 'spelling'
            rationale = f"Very low confidence - possible spelling error or unknown term"

        ui_metadata = {
            'semantic_type_info': semantic_info.ui_information,
            'confidence_level': semantic_info.ui_information['confidence_level'],
            'has_grammar_issues': has_grammar_issues,
            'language': language,
            'analysis_timestamp': time.time()
        }

        return {
            'category': category,
            'score': confidence,
            'rationale': rationale,
            'suggestions': self._generate_suggestions(semantic_info.semantic_type),
            'ui_metadata': ui_metadata
        }

    def _generate_suggestions(self, term_type: str) -> List[str]:
        """Generate suggestions based on term type"""
        suggestions_map = {
            'Entity': ['concept', 'element', 'component'],
            'Event': ['process', 'procedure', 'operation'],
            'Property': ['characteristic', 'attribute', 'quality'],
            'Agent': ['actor', 'performer', 'operator']
        }
        return suggestions_map.get(term_type, ['alternative', 'variant', 'option'])

    def _calculate_statistics(self, analyzed_terms: List[AnalyzedTerm],
                            grammar_analysis: GrammarAnalysis = None) -> AnalysisStatistics:
        """Enhanced statistics calculation"""
        if not analyzed_terms:
            return AnalysisStatistics(
                totalTerms=0, validTerms=0, reviewTerms=0, criticalTerms=0,
                qualityScore=0.0, confidenceMin=0.0, confidenceMax=0.0, coverage=0.0,
                grammarScore=0.0, grammarIssues=0, spellingIssues=0
            )

        total_terms = len(analyzed_terms)
        valid_terms = len([t for t in analyzed_terms if t.classification == 'valid'])
        review_terms = len([t for t in analyzed_terms if t.classification == 'review'])
        critical_terms = len([t for t in analyzed_terms if t.classification == 'critical'])
        spelling_issues = len([t for t in analyzed_terms if t.classification == 'spelling'])

        scores = [t.score for t in analyzed_terms]
        quality_score = (sum(scores) / len(scores)) * 100 if scores else 0.0
        confidence_min = min(scores) if scores else 0.0
        confidence_max = max(scores) if scores else 0.0
        coverage = (valid_terms / total_terms) * 100 if total_terms > 0 else 0.0

        grammar_score = grammar_analysis.grammar_score if grammar_analysis else 100.0
        grammar_issues = grammar_analysis.total_issues if grammar_analysis else 0

        return AnalysisStatistics(
            totalTerms=total_terms,
            validTerms=valid_terms,
            reviewTerms=review_terms,
            criticalTerms=critical_terms,
            qualityScore=quality_score,
            confidenceMin=confidence_min,
            confidenceMax=confidence_max,
            coverage=coverage,
            grammarScore=grammar_score,
            grammarIssues=grammar_issues,
            spellingIssues=spelling_issues
        )

    def _update_performance_stats(self, processing_time: float):
        """Update performance statistics"""
        self.performance_stats['total_analyses'] += 1
        current_avg = self.performance_stats['average_processing_time']
        total_analyses = self.performance_stats['total_analyses']
        new_avg = ((current_avg * (total_analyses - 1)) + processing_time) / total_analyses
        self.performance_stats['average_processing_time'] = new_avg

# Enhanced API Adapter
class EnhancedLexiQAPIAdapter:
    """Enhanced API adapter with all new endpoints"""

    def __init__(self, engine: EnhancedLexiQEngine):
        self.engine = engine

    def analyze_translation(self, translation_content: str, glossary_content: str = "",
                          language: str = 'en', domain: str = 'general',
                          check_grammar: bool = False, user_id: str = None) -> Dict[str, Any]:
        """Enhanced main API endpoint"""
        try:
            result = self.engine.analyze_text(
                text=translation_content,
                glossary_content=glossary_content,
                language=language,
                domain=domain,
                check_grammar=check_grammar,
                user_id=user_id
            )

            return {
                'terms': [asdict(term) for term in result.terms],
                'statistics': asdict(result.statistics),
                'grammar_analysis': result.grammar_analysis,
                'processing_metadata': result.processing_metadata
            }

        except Exception as e:
            logger.error(f"Enhanced API analysis failed: {e}")
            return {
                'error': str(e),
                'terms': [],
                'statistics': {
                    'totalTerms': 0, 'validTerms': 0, 'reviewTerms': 0, 'criticalTerms': 0,
                    'qualityScore': 0.0, 'confidenceMin': 0.0, 'confidenceMax': 0.0, 'coverage': 0.0,
                    'grammarScore': 0.0, 'grammarIssues': 0, 'spellingIssues': 0
                }
            }

    def analyze_grammar(self, text: str, language: str = 'en') -> Dict[str, Any]:
        """Dedicated grammar analysis endpoint"""
        return self.engine.analyze_grammar(text, language)

    def set_user_preference(self, user_id: str, key: str, value: Any) -> bool:
        """Set user preference"""
        return self.engine.cache_service.set_user_preference(user_id, key, value)

    def get_user_preferences(self, user_id: str) -> Dict[str, Any]:
        """Get user preferences"""
        return self.engine.cache_service.get_user_preferences(user_id)